{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joselyne20/USDVER/blob/master/text_data_mgt_assign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qACSe6gwl-D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bb33c7-33fd-4edc-f20d-e58ec5b310bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from string import punctuation\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punc = [i for i in punctuation]\n",
        "punc.append('â€“')"
      ],
      "metadata": {
        "id": "8QP5s5cCsBCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts_TcaRUmSZG",
        "outputId": "59b49cec-022d-4701-b211-0705e5e5f16a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the domains\n",
        "domains = {\n",
        "    'coding': {\n",
        "        'filenames': [f'coding{i}' for i in range(10)],\n",
        "        'links': ['https://www.w3schools.com/python/python_file_open.asp',\n",
        "                  'https://www.geeksforgeeks.org/how-to-read-from-a-file-in-python/',\n",
        "                  'https://www.freecodecamp.org/news/how-to-read-files-in-python/',\n",
        "                  'https://stackoverflow.com/questions/64442598/web-scraping-how-to-get-the-class',\n",
        "                  'https://stackoverflow.com/questions/172720/speeding-up-python',\n",
        "                  'https://stackoverflow.com/questions/59836809/data-visualization-in-python',\n",
        "                  'https://stackoverflow.com/questions/13441788/artificial-intelligence-that-evolves-in-python',\n",
        "                  'https://stackoverflow.com/questions/19231691/python-is-there-an-orm-for-sql-and-nosql',\n",
        "                  'https://stackoverflow.com/questions/62976648/architecture-flask-vs-fastapi',\n",
        "                  'https://stackoverflow.com/questions/23477921/bfs-algorithm-in-python']\n",
        "    },\n",
        "    'Artificial_Intelligence': {\n",
        "        'filenames': [f'Artificial_Intelligence{i}' for i in range(4)],\n",
        "        'links': ['https://www.w3schools.com/python/python_ml_getting_started.asp',\n",
        "                  'https://www.iqsdirectory.com/articles/Artificial_Intelligence/mechanical-components.htmlhttps://www.ibm.com/topics/computer-vision#:~:text=Computer%20vision%20is%20a%20field,recommendations%20based%20on%20that%20information.',\n",
        "                  'https://appinventiv.com/blog/ai-in-gaming/amp/',\n",
        "                  'https://www.simplilearn.com/ai-in-automative-article',\n",
        "                  'https://www.simplilearn.com/tutorials/artificial-intelligence-tutorial/ai-vs-machine-learning-vs-deep-learning',\n",
        "                  'https://www.deeplearning.ai/resources/natural-language-processing/',\n",
        "                  'https://huggingface.co/blog/rlhf',\n",
        "                  'https://www.zdnet.com/article/what-is-the-internet-of-things-everything-you-need-to-know-about-the-iot-right-now/',\n",
        "                  'https://www.avanse.com/blog/robotics-industrial-automation-good-career-path#:~:text=Robotics%20%26%20industrial%20automation%20refers%20to,%2C%20speed%2C%20and%20overall%20performance.',\n",
        "                  'https://www.tech.gov.sg/media/technews/tech-and-education-how-automation-and-ai-is-powering-learning-in-singapore',\n",
        "                  'https://www.ibm.com/blog/the-benefits-of-ai-in-healthcare/']\n",
        "    },\n",
        "    'health': {\n",
        "        'filenames': [f'health{i}' for i in range(2)],\n",
        "        'links': ['https://jamanetwork.com/journals/jama/article-abstract/2811016',\n",
        "                  'https://www.healthxchange.sg/food-nutrition/food-tips/drinking-water-right-time',\n",
        "                  'https://in-part.com/blog/17-top-healthcare-innovations-2023-identified-by-the-global-rd-community/?gclid=CjwKCAjwv-2pBhB-EiwAtsQZFKs-Iq-12u7_eKOdPnhXAPJ46YIoeUQ6rLVt3xBhr2X8e5Anq9wdlhoCu98QAvD_BwE#no17',\n",
        "                  'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6130913/',\n",
        "                  'https://www.medicalnewstoday.com/articles/antibiotics-and-eczema-is-there-a-link',\n",
        "                  'https://my.clevelandclinic.org/health/diseases/7104-diabetes',\n",
        "                  'https://www.samhsa.gov/mental-health',\n",
        "                  'https://www.a-star.edu.sg/gis/our-science/disease-research-at-gis/cardiovascular-disease-research',\n",
        "                  'https://www.cancer.gov/news-events/cancer-currents-blog',\n",
        "                  'https://www.activesgcircle.gov.sg/activehealth/read/nutrition/what-is-good-nutrition-and-why-is-it-important',\n",
        "                  'https://apac.mykidneyjourney.com/en-SG/dialysis?utm_source=SEM&utm_medium=CPC&utm_campaign=SG_SEM_2023&utm_content=AboutDialysis_RSA&gclid=Cj0KCQjw4vKpBhCZARIsAOKHoWQ29x_VjJYxVX-TTmUVphswT1i38BMewld33PNYv3j6F-2-nnJdPA8aAiYuEALw_wcB']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Create DataFrames for each domain\n",
        "dataframes = {}\n",
        "\n",
        "for domain in domains.keys():\n",
        "    data = {'Original Text': [], 'Tokenized Text': [], 'Stemmed Text': []}\n",
        "    sizes = [[],[],[]]\n",
        "    dfs = []\n",
        "    for index, (filename, link) in enumerate(zip(domains[domain]['filenames'], domains[domain]['links'])):\n",
        "        # Visit the URL and read the content\n",
        "        page = requests.get(link)\n",
        "        contents = bs(page.content, 'html.parser')\n",
        "        paragraphs = contents.find_all('p')\n",
        "        original_text = []\n",
        "        tokenized_text = []\n",
        "        stemmed_text = []\n",
        "        patten = re.compile(\"\\W|\\s|\\d\")\n",
        "\n",
        "        for i, paragraph in enumerate(paragraphs):\n",
        "            text = paragraph.text\n",
        "            temp = [[],[],[]]\n",
        "            decision = False\n",
        "            if text.split():\n",
        "              tokens = [word if word else '' for word in word_tokenize(text) if not patten.findall(word)]\n",
        "              stemmed_tokens = [stemmer.stem(word) if word else '' for word in tokens]\n",
        "              original_text.append(text)\n",
        "              tokenized_text.append(' '.join(tokens))\n",
        "              stemmed_text.append(' '.join(stemmed_tokens))\n",
        "              decision = True\n",
        "\n",
        "              temp[0].append(len(text.split()))\n",
        "              temp[1].append(len(tokens))\n",
        "              temp[2].append(len(stemmed_tokens))\n",
        "            if temp and decision:\n",
        "              sizes[0].append(temp[0][0])\n",
        "              sizes[1].append(temp[1][0])\n",
        "              sizes[2].append(temp[2][0])\n",
        "\n",
        "        if original_text and decision:\n",
        "          data['Original Text'].append('\\n'.join(original_text))\n",
        "          data['Tokenized Text'].append('\\n'.join(tokenized_text))\n",
        "          data['Stemmed Text'].append('\\n'.join(stemmed_text))\n",
        "\n",
        "        full_doc = zip(original_text, tokenized_text, stemmed_text, sizes[0], sizes[1], sizes[2])\n",
        "        df2 = pd.DataFrame(full_doc, columns = ['Original', 'Tokenized',\n",
        "                                        'Stemmed', 'Original size',\n",
        "                                        'Tokens size', 'Stemmed size'])\n",
        "        dfs.append(df2)\n",
        "        dfs.append(pd.DataFrame([['***'*3, '***'*3, '***'*3, '***'*3, '***'*3, '***'*3]], columns = ['Original', 'Tokenized',\n",
        "                                        'Stemmed', 'Original size',\n",
        "                                        'Tokens size', 'Stemmed size']))\n",
        "    combined_dfs = pd.concat(dfs)\n",
        "    combined_dfs.to_csv(f'{domain}@.csv')\n",
        "    # Create a DataFrame for the domain\n",
        "    df = pd.DataFrame(data)\n",
        "    dataframes[domain] = df\n",
        "\n",
        "# # Save DataFrames to CSV files\n",
        "# for domain, df in dataframes.items():\n",
        "#     df.to_csv(f'{domain}_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "I2itP3jhmBXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_token_length = max(len(token) for token in original_text)\n",
        "max_token_length"
      ],
      "metadata": {
        "id": "3udlZ7zu7Z7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "fleLhwOq8yKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_doc = zip(original_text, tokenized_text, stemmed_text, sizes[0], sizes[1], sizes[2])\n",
        "df2 = pd.DataFrame(full_doc, columns = ['Original', 'Tokenized',\n",
        "                                        'Stemmed', 'Original size',\n",
        "                                        'Tokens size', 'Stemmed size'])\n",
        "df2.to_csv('data_comb.csv')"
      ],
      "metadata": {
        "id": "if6tqhsWofhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len('Drinking Water at the Right Time')"
      ],
      "metadata": {
        "id": "0sNllRh1xjvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji = \"\\U0001F197\"\n",
        "\n",
        "print(emoji)"
      ],
      "metadata": {
        "id": "bgeaTXtDIPKC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}